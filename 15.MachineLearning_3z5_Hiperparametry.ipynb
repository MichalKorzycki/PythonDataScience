{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warsztaty Python w Data Science\n",
    "\n",
    "---\n",
    "\n",
    "## Machine Learning - część 3 z 5. Optymalizacja Hiperparametrów. Pipelines\n",
    "\n",
    "- ### Zmiana scoringu\n",
    "- ### Zmiana regresora\n",
    "- ### Porównanie regresorów. Hiperparametry\n",
    "- ### Skalowanie\n",
    "- ### Pipeline dla tekstu\n",
    "- ### Kompozytowy Pipeline\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from numpy import log2\n",
    "\n",
    "data = pd.read_csv('data/adverts_29_04.csv', sep=';')\n",
    "data['cena_za_metr'] = data['Cena'] / data['Wielkość (m2)']\n",
    "data[\"log\"] = data['Wielkość (m2)'].apply(lambda x: log2(x))\n",
    "data[\"clog\"] = data['Cena'].apply(lambda x: log2(x))\n",
    "data = data.dropna(subset=['cena_za_metr'])\n",
    "df = data.drop(['Cena', 'Data dodania'], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dum_df = pd.get_dummies(df, columns=['Lokalizacja', 'Na sprzedaż przez', 'Rodzaj nieruchomości', 'Liczba pokoi', 'Liczba łazienek', 'Parking'])\n",
    "dum_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dum_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dum_df = pd.get_dummies(df, drop_first = True, columns=['Lokalizacja', 'Na sprzedaż przez', 'Rodzaj nieruchomości', 'Liczba pokoi', 'Liczba łazienek', 'Parking'])\n",
    "dum_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "y = dum_df['cena_za_metr']\n",
    "X = dum_df.drop(['opis', 'cena_za_metr'], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Walidacja krzyżowa\n",
    "\n",
    "![Walidacja krzyżowa](img\\xvi.png)\n",
    "\n",
    "https://scikit-learn.org/stable/modules/cross_validation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=17)\n",
    "scores = cross_val_score(LinearRegression(), X_train, y_train, cv=5)\n",
    "print(list(scores))\n",
    "print()\n",
    "print(\"Mean r^2: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zmiana techniki scoringu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "\n",
    "\n",
    "scores = cross_val_score(LinearRegression(), X_train, y_train, scoring=make_scorer(mean_squared_error), cv=5)\n",
    "print(list(scores))\n",
    "print()\n",
    "print(\"Mean square error: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zmiana regresora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "scores = cross_val_score(SVR(kernel='linear', C=10), X_train, y_train, cv=5)\n",
    "print(list(scores))\n",
    "print()\n",
    "print(\"Mean r^2: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "scores = cross_val_score(SVR(kernel='linear', C=100), X_train, y_train, cv=5)\n",
    "print(list(scores))\n",
    "print()\n",
    "print(\"Mean r^2: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hiperparametr:\n",
    "### __*C*__ - współczynnik regularyzacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.style.use(\"dark_background\")\n",
    "plt.figure(figsize=(10,6))\n",
    "x = np.linspace(-0.5, 2, 100)\n",
    "plt.plot(x, x*(5*x-1)*(x-2))\n",
    "y = np.linspace(0, 1.4, 3)\n",
    "plt.scatter(y, -3.4*y);\n",
    "z = np.linspace(-0.5, 2, 50)\n",
    "plt.plot(z, -3.4*z-0.3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.svm import SVR\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scale',  'passthrough'),\n",
    "    ('regression', SVR())\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.set_params(regression__C=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = dict(regression__C=[0.1, 10, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid_search = GridSearchCV(pipe, param_grid, verbose=1, cv=3)\n",
    "grid_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalers\n",
    "- StandardScaler (standardyzacja - odejmuję średnia, dzieli przez wariancję)\n",
    "- Normalizer (normalizacja - dzieli przez długość - sprowadza do wektora o normie 1)\n",
    "- RobustScaler (odejmuje medianę i skaluje kwartylami)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "param_grid = dict( \n",
    "    scale=['passthrough', StandardScaler(), Normalizer()],\n",
    "    regression__C=[ 10, 100],\n",
    "    regression__kernel=['linear']\n",
    ")\n",
    "                  \n",
    "print(param_grid)\n",
    "\n",
    "###################################\n",
    "\n",
    "grid_search = GridSearchCV(pipe, param_grid, verbose=1, cv=5, n_jobs=2)\n",
    "\n",
    "###################################\n",
    "\n",
    "t0 = time()\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n",
    "print(\"Best parameters set:\")\n",
    "print(grid_search.best_estimator_)\n",
    "print()\n",
    "print(f\"Best score: {grid_search.best_score_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 5 - Walidacja krzyżowa\n",
    "- 2 - parametry $C$\n",
    "- 3 - scaler\n",
    "\n",
    "$ 5 * 2 * 3 = 30 $ przebiegów"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline dla tekstu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "tfs = tfidf.fit_transform(df[\"opis\"])\n",
    "tfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "                ('selector', ItemSelector(key='opis')),\n",
    "                ('tfidf', TfidfVectorizer()),\n",
    "                ('best', TruncatedSVD(n_components=250)),\n",
    "                ('linear', LinearRegression())\n",
    "            ])\n",
    "\n",
    "y = dum_df['cena_za_metr']\n",
    "X = dum_df.drop(['cena_za_metr'], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1)\n",
    "scores = cross_val_score(pipeline, X_train, y_train, cv=3)\n",
    "print(list(scores))\n",
    "print()\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from time import time\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]\n",
    "\n",
    "parameters = parameters = {\n",
    "    'best__n_components': (750,1000),\n",
    "    'svr__C': (100,1000),\n",
    "    'svr__kernel':('linear', 'rbf')\n",
    "}\n",
    "\n",
    "pipeline = Pipeline([\n",
    "                ('selector', ItemSelector(key='opis')),\n",
    "                ('tfidf', TfidfVectorizer()),\n",
    "                ('best', TruncatedSVD()),\n",
    "                ('svr', SVR())\n",
    "            ])\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, verbose=1, cv=5, n_jobs=2)\n",
    "\n",
    "\n",
    "y = dum_df['cena_za_metr']\n",
    "X = dum_df.drop(['cena_za_metr'], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=17)\n",
    "\n",
    "t0 = time()\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from operator import itemgetter\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=2)\n",
    "X = vectorizer.fit_transform(X_train['opis'])\n",
    "\n",
    "sorted(zip(vectorizer.idf_, vectorizer.get_feature_names()), key=itemgetter(0), reverse=True)[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import sys\n",
    "import re\n",
    "import re\n",
    "\n",
    "splitter = re.compile(r'[^ąąćęńłóóśśżżź\\w]+')\n",
    "isnumber = re.compile(r'[0-9]')\n",
    "\n",
    "f = gzip.open('data/odm.txt.gz', 'rt', encoding='utf-8')\n",
    "dictionary = {}\n",
    "set_dict= set()\n",
    "\n",
    "for x in f:\n",
    "    t = x.strip().split(',')\n",
    "    tt = [ x.strip().lower() for x in t]\n",
    "    for w in tt:\n",
    "        set_dict.add(w)\n",
    "        dictionary[w]=tt[0]\n",
    "\n",
    "def lematize(w):\n",
    "    w = w.replace('ą','ą')\n",
    "    w = w.replace('ó','ó')\n",
    "    w = w.replace('ę','ę')\n",
    "    w = w.replace('ż','ż')\n",
    "    return dictionary.get(w,w)\n",
    "\n",
    "opis1 = dum_df['opis'][0]\n",
    "\n",
    "\n",
    "\n",
    "raw_corpus=[]\n",
    "n=0\n",
    "\n",
    "for i in dum_df.iterrows():\n",
    "    n+=1\n",
    "    l = list(splitter.split(i[1][1]))\n",
    "    raw_corpus.append(l)\n",
    "\n",
    "    \n",
    "all_words = []\n",
    "for t in raw_corpus:\n",
    "    all_words[0:0] = t\n",
    "\n",
    "words = {}\n",
    "for w in all_words:\n",
    "    rec = words.get(w.lower(), {'upper':0, 'lower': 0})\n",
    "    if w.lower()==w or w.upper()==w:\n",
    "        rec['lower'] = rec['lower'] +1\n",
    "    else: \n",
    "        rec['upper'] = rec['upper'] +1\n",
    "    words[w.lower()] = rec\n",
    "\n",
    "raw_stop_words = [ x for x in words.keys() if words[x]['upper']>=words[x]['lower']*4 ]   \n",
    "\n",
    "set_raw_stop_words = set(raw_stop_words)\n",
    "\n",
    "def preprocessing(opis, filter_raw=True, filter_dict=True):\n",
    "    opis = str(opis)\n",
    "    tokenized = splitter.split(opis)\n",
    "    l = list(tokenized)\n",
    "    l = [ x.lower() for x in l ]\n",
    "    l = [ x for x in l if len(x) > 2]\n",
    "    l = [ x for x in l if x.find('_') < 0]\n",
    "    l = [ x for x in l if isnumber.search(x) is None ]\n",
    "    if filter_raw: l = [ x for x in l if x not in set_raw_stop_words ]\n",
    "    if filter_dict: l = [ x for x in l if x in set_dict ]\n",
    "    l = [ lematize(x) for x in l ]\n",
    "    l = [ x for x in l if len(x) > 2]\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opis1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preprocessing(opis1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preprocessing(opis1, filter_raw=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preprocessing(opis1, filter_dict=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preprocessing(opis1, filter_raw=False, filter_dict=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dum_df[\"opisTT\"] = dum_df[\"opis\"].apply(lambda x: ' '.join(preprocessing(x,filter_raw=True, filter_dict=True)))\n",
    "dum_df[\"opisTF\"] = dum_df[\"opis\"].apply(lambda x: ' '.join(preprocessing(x,filter_raw=True, filter_dict=False)))\n",
    "dum_df[\"opisFT\"] = dum_df[\"opis\"].apply(lambda x: ' '.join(preprocessing(x,filter_raw=False, filter_dict=True)))\n",
    "dum_df[\"opisFF\"] = dum_df[\"opis\"].apply(lambda x: ' '.join(preprocessing(x,filter_raw=False, filter_dict=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Pipeline kompozytowy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from time import time\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer, RobustScaler\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key=''):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]\n",
    "\n",
    "class ItemUnSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, keys=[]):\n",
    "        self.keys = keys\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict.drop(self.keys, axis=1)\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "   ('union', \n",
    "        FeatureUnion(\n",
    "            transformer_list=[\n",
    "                ('table', \n",
    "                    Pipeline([\n",
    "                        ('selector1', ItemUnSelector(keys=['opis', 'opisTT', 'opisTF', 'opisFT', 'opisFF'])),\n",
    "                        ('scaler1', 'passthrough')\n",
    "                    ])\n",
    "                ),\n",
    "                ('description', \n",
    "                    Pipeline([\n",
    "                        ('selector2', ItemSelector()),\n",
    "                        ('tfidf', TfidfVectorizer()),\n",
    "                        ('best', TruncatedSVD()),\n",
    "                        ('scaler2', 'passthrough')\n",
    "                    ])\n",
    "                )\n",
    "            ]\n",
    "        )    \n",
    "\n",
    "   ),\n",
    "   ('regressor', \n",
    "        TransformedTargetRegressor()\n",
    "    )\n",
    "])\n",
    "\n",
    "parameters = parameters = {\n",
    "    'union__transformer_weights': [  { 'table': 1.0, 'description': 1.0}],\n",
    "\n",
    "    'union__description__best__n_components': (700,),\n",
    "    'union__description__tfidf__min_df': (3,),\n",
    "    'union__description__tfidf__binary': (True,),\n",
    "    'union__description__selector2__key': [ 'opisFF'] ,\n",
    "    \n",
    "    'union__table__scaler1': [ RobustScaler()],\n",
    "    'union__description__scaler2': [ RobustScaler(with_centering=False)],\n",
    "    \n",
    "    'regressor': [ GradientBoostingRegressor()] ,\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, verbose=1, cv=10, n_jobs=4)\n",
    "\n",
    "\n",
    "y = dum_df['cena_za_metr']\n",
    "X = dum_df.drop(['cena_za_metr'], axis=1)\n",
    "\n",
    "t0 = time()\n",
    "grid_search.fit(X, y)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "print(f'Best score: {grid_search.best_score_}')\n",
    "\n",
    "print(\"Best parameters set:\")\n",
    "print()\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problemy z trenowaniem modelu\n",
    "\n",
    "\n",
    "### To ile tych prób możemy mieć ?\n",
    "\n",
    "- 3 zestawy wag `union`\n",
    "- 6 zestawów wymiarów SVD\n",
    "- 6 zestawów parametrów TF-IDF\n",
    "- 4 zbiory danych tekstowych\n",
    "- 4 mechanizmy skalowania części `table`\n",
    "- 4 mechanizmy skalowania części `description`\n",
    "- 3 regresory\n",
    "- 10 walidacji krzyżowych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3*6*6*4*4*4*3*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3*6*6*4*4*4*3*10/3/60/24"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
